{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Resistance Aggregation\n",
    "\n",
    "**Authors:** Matear, L. (2018)\n",
    "**Contact Email:** Liam.Matear@jncc.gov.uk\n",
    "**Version Control:** 0.1 \n",
    "\n",
    "**Description:** This script allows the user to aggregate resistance data across varying tiers of EUNIS hierarchies. The main purpose of this code is to develop a mechanism through which the user can identify which completed assessments sit within which tiers of the EUNIS hierarchy. The outputs of these analyses should enable the user to map the spatial distribution of assessments made at EUNIS Levels 5 and 6 at less detailed resolutions (e.g. EUNIS Level 2 and / or EUNIS Level 3).\n",
    "\n",
    "For any enquiries please contact the Author by email: Liam.Matear@jncc.gov.uk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Introduction\n",
    "\n",
    "This script allows the user to aggregate resistance data across varying tiers of EUNIS hierarchies. The main purpose of this code is to develop a mechanism through which the user can identify which completed assessments sit within which tiers of the EUNIS hierarchy. The outputs of these analyses should enable the user to map the spatial distribution of assessments made at EUNIS Levels 5 and 6 at less detailed resolutions (e.g. EUNIS Level 2 and / or EUNIS Level 3).\n",
    "\n",
    "## Aggregation Process Outline\n",
    "\n",
    "See aggregation process infographic file within the 'Sensitivity Assessments' folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2a: Initial setup and data import \n",
    "\n",
    "Import libraries used within the script, assign a working directory and import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all Python libraries required \n",
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "# Set working directory for file access\n",
    "os.chdir(r'ENTER FILE PATH HERE')\n",
    "\n",
    "# Load required data and assign to object oriented variables \n",
    "bioregions = pd.read_excel('ENTER FILE NAME HERE', 'ENTER TARGET TAB HERE')\n",
    "maresa = pd.read_excel('ENTER FILE NAME HERE', 'ENTER TARGET TAB HERE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2b: Defining functions (data formatting)\n",
    "\n",
    "Define all functions which are required within the script to format data for aggregation process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions required for data manipulation \n",
    "\n",
    "# Function Title: df_clean \n",
    "def df_clean(df):\n",
    "    \"\"\"User defined function to refine dataset - remove whitespace\n",
    "     and Inshore / No Biotope Presence data\"\"\"\n",
    "    # Toggle this on / off to get offshore or all data together\n",
    "    df.drop(df[df.BiotopePresence == 'Inshore only'].index, inplace=True)\n",
    "    # Refine dataset to only include data for which BiotopePresence == 'Poss' or 'Yes'\n",
    "    df.drop(df[df.BiotopePresence == 'No'].index, inplace=True)\n",
    "    df['EUNIS_Code'].str.strip()\n",
    "    df['Resistance'].replace([\"Not relevant (NR)\"], \"Not relevant\", inplace=True)\n",
    "    df['Resistance'].replace([\"No evidence (NEv)\"], \"No evidence\", inplace=True)\n",
    "    df['Resistance'].replace([\"Not assessed (NA)\"], \"Not assessed\", inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function Title: unwanted_char\n",
    "def unwanted_char(df, column):\n",
    "    \"\"\"User defined function to refine dataset - remove unwanted special characters and acronyms from resistance\n",
    "    scores. User must pass the DataFrame and the column (as a string) as the arguments to the parentheses of the\n",
    "    function\"\"\"\n",
    "    for eachField in df[column]:\n",
    "        eachField.replace(r\"\\\\s*zz(][^\\\\]+\\\\)\", \"\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function Title: eunis_col\n",
    "def eunis_col(row):\n",
    "    \"\"\"User defined function to pull out all entries in EUNIS_Code column and create returns based on string\n",
    "    slices of the EUNIS data. This must be used with df.apply() and a lambda function.\n",
    "    \n",
    "    e.g. fact_tbl[['Level_1', 'Level_2', 'Level_3',\n",
    "          'Level_4', 'Level_5', 'Level_6']] = fact_tbl.apply(lambda row: pd.Series(eunis_col(row)), axis=1)\"\"\"\n",
    "\n",
    "    # Create object oriented variable to store EUNIS_Code data\n",
    "    ecode = row['EUNIS_Code']\n",
    "    # Create if / elif conditions to produce response dependent on the string length of the inputted data.\n",
    "    if len(ecode) == 1:\n",
    "        return ecode[0:1], None, None, None, None, None\n",
    "    elif len(ecode) == 2:\n",
    "        return ecode[0:1], ecode[0:2], None, None, None, None\n",
    "    elif len(ecode) == 4:\n",
    "        return ecode[0:1], ecode[0:2], ecode[0:4], None, None, None\n",
    "    elif len(ecode) == 5:\n",
    "        return ecode[0:1], ecode[0:2], ecode[0:4], ecode[0:5], None, None\n",
    "    elif len(ecode) == 6:\n",
    "        return ecode[0:1], ecode[0:2], ecode[0:4], ecode[0:5], ecode[0:6], None\n",
    "    elif len(ecode) == 7:\n",
    "        return ecode[0:1], ecode[0:2], ecode[0:4], ecode[0:5], ecode[0:6], ecode[0:7]\n",
    "\n",
    "\n",
    "# Function Title: eunis_lvl\n",
    "def eunis_lvl(row):\n",
    "    \"\"\"User defined function to pull out all data from the column 'EUNIS_Code' and return an integer dependant on the EUNIS \n",
    "       level in response\"\"\"\n",
    "\n",
    "    # Create object oriented variable to store EUNIS_Code data\n",
    "    ecode = row['EUNIS_Code']\n",
    "    # Create if / elif conditions to produce response dependent on the string length of the inputted data\n",
    "    if len(ecode) == 1:\n",
    "        return '1'\n",
    "    elif len(ecode) == 2:\n",
    "        return '2'\n",
    "    elif len(ecode) == 4:\n",
    "        return '3'\n",
    "    elif len(ecode) == 5:\n",
    "        return '4'\n",
    "    elif len(ecode) == 6:\n",
    "        return '5'\n",
    "    elif len(ecode) == 7:\n",
    "        return '6'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2c: Defining functions (aggregation process)\n",
    "\n",
    "Define all functions which are required within the script to execute aggregation process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Title: counter\n",
    "def counter(value):\n",
    "    \"\"\"Count the total no. of occurrences of each sensitivity (high, medium, low, not sensitive, not relevant,\n",
    "      no evidence, not assessed, unknown\n",
    "      Return values to be assigned to new columns through lambda function\"\"\"\n",
    "    counthigh = value.count('High')\n",
    "    countmedium = value.count('Medium')\n",
    "    countlow = value.count('Low')\n",
    "    countns = value.count('Not sensitive')\n",
    "    countnr = value.count('Not relevant')\n",
    "    countne = value.count('No evidence')\n",
    "    countna = value.count('Not assessed')\n",
    "    countuk = value.count('Unknown')\n",
    "    return counthigh, countmedium, countlow, countns, countnr, countne, countna, countuk\n",
    "\n",
    "\n",
    "# Function Title: replacer\n",
    "def replacer(value, repstring):\n",
    "    \"\"\"Perform string replace on each sensitivity count column (one set of duplicates only)\"\"\"\n",
    "    if value == 0:\n",
    "        return 'NA'\n",
    "    elif value != 0:\n",
    "        return repstring\n",
    "\n",
    "    \n",
    "# Function Title: create_resistance\n",
    "def create_Resistance(df):\n",
    "    \"\"\"Series of conditional statements which return a string value of all assessment values\n",
    "    contained within individual columns\"\"\"\n",
    "    # Create object oriented variable for each column of data from DataFrame (assessed only)\n",
    "    high = df['High']\n",
    "    med = df['Medium']\n",
    "    low = df['Low']\n",
    "    nsens = df['Not sensitive']\n",
    "    # Create object oriented variable for each column of data from DataFrame (not assessment criteria only)\n",
    "    nrel = df['Not relevant']\n",
    "    nev = df['No evidence']\n",
    "    n_ass = df['Not assessed']\n",
    "    un = df['Unknown']\n",
    "\n",
    "    # Create empty list for all string values to be appended into - this will be assigned to each field when data are\n",
    "    # iterated through using the lambdas function which follows immediately after this function\n",
    "    value = []\n",
    "    # Create series of conditional statements to append string values into the empty list ('value') if conditional\n",
    "    # statements are fulfilled\n",
    "    if 'High' in high:\n",
    "        h = 'High'\n",
    "        value.append(h)\n",
    "    if 'Medium' in med:\n",
    "        m = 'Medium'\n",
    "        value.append(m)\n",
    "    if 'Low' in low:\n",
    "        lo = 'Low'\n",
    "        value.append(lo)\n",
    "    if 'Not sensitive' in nsens:\n",
    "        ns = 'Not sensitive'\n",
    "        value.append(ns)\n",
    "    if 'Not relevant' in nrel:\n",
    "        nr = 'Not relevant'\n",
    "        value.append(nr)\n",
    "    if 'No evidence' in nev:\n",
    "        ne = 'No evidence'\n",
    "        value.append(ne)\n",
    "    if 'Not assessed' in n_ass:\n",
    "        nass = 'Not assessed'\n",
    "        value.append(nass)\n",
    "    if 'Unknown' in un:\n",
    "        unk = 'Unknown'\n",
    "        value.append(unk)\n",
    "    s = ', '.join(value)\n",
    "    return str(s)\n",
    "\n",
    "\n",
    "# Function Title: final_Resistance\n",
    "def final_Resistance(df):\n",
    "    \"\"\"Create a return of a string value which gives final sensitivity score dependent on conditional statements\"\"\"\n",
    "    # Create object oriented variable for each column of data from DataFrame (assessed only)\n",
    "    high = df['High']\n",
    "    med = df['Medium']\n",
    "    low = df['Low']\n",
    "    nsens = df['Not sensitive']\n",
    "    # Create object oriented variable for each column of data from DataFrame (not assessment criteria only)\n",
    "    nrel = df['Not relevant']\n",
    "    nev = df['No evidence']\n",
    "    n_ass = df['Not assessed']\n",
    "    un = df['Unknown']\n",
    "\n",
    "    # Create empty list for all string values to be appended into - this will be assigned to each field when data are\n",
    "    # iterated through using the lambdas function which follows immediately after this function\n",
    "    value = []\n",
    "    # Create series of conditional statements to append string values into the empty list ('value') if conditional\n",
    "    # statements are fulfilled\n",
    "    if 'High' in high:\n",
    "        h = 'High'\n",
    "        value.append(h)\n",
    "    if 'Medium' in med:\n",
    "        m = 'Medium'\n",
    "        value.append(m)\n",
    "    if 'Low' in low:\n",
    "        lo = 'Low'\n",
    "        value.append(lo)\n",
    "    if 'Not sensitive' in nsens:\n",
    "        ns = 'Not sensitive'\n",
    "        value.append(ns)\n",
    "    if 'High' not in high and 'Medium' not in med and 'Low' not in low and 'Not sensitive' not in nsens:\n",
    "        if 'Not relevant' in nrel:\n",
    "            nr = 'Not relevant'\n",
    "            value.append(nr)\n",
    "        if 'No evidence' in nev:\n",
    "            ne = 'No evidence'\n",
    "            value.append(ne)\n",
    "        if 'Not assessed' in n_ass:\n",
    "            nass = 'Not assessed'\n",
    "            value.append(nass)\n",
    "    if 'NA' in high and 'NA' in med and 'NA' in low and 'NA' in nsens and 'NA' in nrel and 'NA' in nev and \\\n",
    "            'NA' in n_ass:\n",
    "        un = 'Unknown'\n",
    "        value.append(un)\n",
    "    s = ', '.join(value)\n",
    "    return str(s)\n",
    "\n",
    "\n",
    "# Function Title: combine_assessedcounts\n",
    "def combine_assessedcounts(df):\n",
    "    \"\"\"Conditional statements which combine assessed count data and return as string value\"\"\"\n",
    "    # Create object oriented variable for each column of data from DataFrame (assessed only)\n",
    "    high = df['High']\n",
    "    med = df['Medium']\n",
    "    low = df['Low']\n",
    "    nsens = df['Not sensitive']\n",
    "    # Create object oriented variable for each column of data from DataFrame (not assessment criteria only)\n",
    "    nrel = df['Not relevant']\n",
    "    nev = df['No evidence']\n",
    "    n_ass = df['Not assessed']\n",
    "    un = df['Unknown']\n",
    "\n",
    "    # Create empty list for all string values to be appended into - this will be assigned to each field when data are\n",
    "    # iterated through using the lambdas function which follows immediately after this function\n",
    "    value = []\n",
    "    # Create series of conditional statements to append string values into the empty list ('value') if conditional\n",
    "    # statements are fulfilled\n",
    "    if 'High' in high:\n",
    "        h = 'H(' + str(df['Count_High']) + ')'\n",
    "        value.append(h)\n",
    "    if 'Medium' in med:\n",
    "        m = 'M(' + str((df['Count_Medium'])) + ')'\n",
    "        value.append(m)\n",
    "    if 'Low' in low:\n",
    "        lo = 'L(' + str(df['Count_Low']) + ')'\n",
    "        value.append(lo)\n",
    "    if 'Not sensitive' in nsens:\n",
    "        ns = 'NS(' + str(df['Count_NotSensitive']) + ')'\n",
    "        value.append(ns)\n",
    "    if 'Not relevant' in nrel:\n",
    "        nr = 'NR(' + str(df['Count_NotRel']) + ')'\n",
    "        value.append(nr)\n",
    "    if 'NA' in high and 'NA' in med and 'NA' in low and 'NA' in nsens and 'NA' in nrel:\n",
    "        # if 'NA' in high and 'NA' in med and 'NA' in low and 'NA' in nsens:\n",
    "        # if 'Not relevant' in nrel:\n",
    "        #     nr = 'Not Applicable'\n",
    "        #     value.append(nr)\n",
    "        if 'No evidence' in nev:\n",
    "            ne = 'Not Applicable'\n",
    "            value.append(ne)\n",
    "        if 'Not assessed' in n_ass:\n",
    "            nass = 'Not Applicable'\n",
    "            value.append(nass)\n",
    "        if 'Unknown' in un:\n",
    "            unk = 'Not Applicable'\n",
    "            value.append(unk)\n",
    "    s = ', '.join(value)\n",
    "    return str(s)\n",
    "\n",
    "\n",
    "# Function Title: combine_unassessedcounts\n",
    "def combine_unassessedcounts(df):\n",
    "    \"\"\"Conditional statements which combine unassessed count data and return as string value\"\"\"\n",
    "    # Create object oriented variable for each column of data from DataFrame (assessed only)\n",
    "    # Create object oriented variable for each column of data from DataFrame (not assessment criteria only)\n",
    "    nrel = df['Not relevant']\n",
    "    nev = df['No evidence']\n",
    "    n_ass = df['Not assessed']\n",
    "    un = df['Unknown']\n",
    "\n",
    "    # Create empty list for all string values to be appended into - this will be assigned to each field when data are\n",
    "    # iterated through using the lambdas function which follows immediately after this function\n",
    "\n",
    "    values = []\n",
    "\n",
    "    # Create series of conditional statements to append string values into the empty list ('value') if conditional\n",
    "    # statements are fulfilled\n",
    "\n",
    "    # if 'Not relevant' in nrel:\n",
    "    #     nr = 'NR(' + str(df['Count_NotRel']) + ')'\n",
    "    #     values.append(nr)\n",
    "    if 'No evidence' in nev:\n",
    "        ne = 'NE(' + str(df['Count_NoEvidence']) + ')'\n",
    "        values.append(ne)\n",
    "    if 'Not assessed' in n_ass:\n",
    "        na = 'NA(' + str(df['Count_NotAssessed']) + ')'\n",
    "        values.append(na)\n",
    "    if 'Unknown' in un:\n",
    "        unk = 'UN(' + str(df['Count_Unknown']) + ')'\n",
    "        values.append(unk)\n",
    "    # if 'NA' in nrel and 'NA' in nev and 'NA' in n_ass and 'NA' in un:\n",
    "    if 'NA' in nev and 'NA' in n_ass and 'NA' in un:\n",
    "            napp = 'Not Applicable'\n",
    "            values.append(napp)\n",
    "    s = ', '.join(values)\n",
    "    return str(s)\n",
    "\n",
    "\n",
    "# Function Title: create_confidence\n",
    "def create_confidence(df):\n",
    "    \"\"\"Divide the total assessed counts by the total count of all data and return as numerical value\"\"\"\n",
    "    # Pull in assessed values counts\n",
    "    count_high = df['Count_High']\n",
    "    count_med = df['Count_Medium']\n",
    "    count_low = df['Count_Low']\n",
    "    count_ns = df['Count_NotSensitive']\n",
    "\n",
    "    # Pull in unassessed values counts\n",
    "    count_nr = df['Count_NotRel']\n",
    "    count_ne = df['Count_NoEvidence']\n",
    "    count_na = df['Count_NotAssessed']\n",
    "    count_unk = df['Count_Unknown']\n",
    "\n",
    "    # Create ratio calculation - count_nr added to total ass and removed from total\n",
    "    # total_ass = count_high + count_med + count_low + count_ns + count_nr\n",
    "\n",
    "    # Create new test version without NR\n",
    "    total_ass = count_high + count_med + count_low + count_ns\n",
    "    total = total_ass + count_ne + count_na + count_unk\n",
    "\n",
    "    return total_ass / total if total else 0\n",
    "\n",
    "\n",
    "# Function Title: categorise_confidence\n",
    "def categorise_confidence(df, column):\n",
    "    \"\"\"Partition and categorise confidence values by quantile intervals\"\"\"\n",
    "    if column == 'L2_AssessmentConfidence':\n",
    "        value = df[column]\n",
    "        if value < 0.33:\n",
    "            return 'Low'\n",
    "        elif value >= 0.33 and value < 0.66:\n",
    "            return ' Medium'\n",
    "        elif value >= 0.66:\n",
    "            return 'High'\n",
    "    elif column == 'L3_AssessmentConfidence':\n",
    "        value = df[column]\n",
    "        if value < 0.33:\n",
    "            return 'Low'\n",
    "        elif value >= 0.33 and value < 0.66:\n",
    "            return ' Medium'\n",
    "        elif value >= 0.66:\n",
    "            return 'High'\n",
    "    elif column == 'L4_AssessmentConfidence':\n",
    "        value = df[column]\n",
    "        if value < 0.33:\n",
    "            return 'Low'\n",
    "        elif value >= 0.33 and value < 0.66:\n",
    "            return ' Medium'\n",
    "        elif value == 0.66:\n",
    "            return 'High'\n",
    "    elif column == 'L5_AssessmentConfidence':\n",
    "        return 'NA'\n",
    "    \n",
    "    \n",
    "# Function Title: column4\n",
    "def column4(df):\n",
    "    \"\"\"Sample Level_5 column and return string variables sliced within the range [0:5]\"\"\"\n",
    "    value = df['Level_5']\n",
    "    sample = value[0:5]\n",
    "    return sample\n",
    "\n",
    "\n",
    "# Function Title: column3\n",
    "def column3(df, column):\n",
    "    \"\"\"User defined function to sample Level_4 column and return string variables sliced within the range [0:4]\"\"\"\n",
    "    value = df[column]\n",
    "    sample = value[0:4]\n",
    "    return sample\n",
    "\n",
    "\n",
    "# Function Title: column2\n",
    "def column2(df, column):\n",
    "    \"\"\"User defined function to sample Level_4 column and return string variables sliced within the range [0:4]\"\"\"\n",
    "    value = df[column]\n",
    "    sample = value[0:2]\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2d: Data formatting \n",
    "\n",
    "Remove unnecessary whitespace, rename Bioregions column and merge data sets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename bioregions column to facilitate merge \n",
    "bioregions.rename(columns={'HabitatCode': 'EUNIS_Code'}, inplace=True)\n",
    "\n",
    "# Merge data to create fact_tbl dataset \n",
    "fact_tbl = pd.merge(bioregions, maresa, on='EUNIS_Code')\n",
    "\n",
    "# Remove unwanted data by passing the fact_tbl DF to the df_clean() function\n",
    "df_clean(fact_tbl)\n",
    "\n",
    "# Fill NaN values with empty string values to allow for string formatting with unwanted_char() function\n",
    "fact_tbl['Resistance'].fillna(value='', inplace=True)\n",
    "\n",
    "# Remove unwanted special characters from data by passing the fact_tbl to the unwanted_char() function\n",
    "fact_tbl = unwanted_char(fact_tbl, 'Resistance')\n",
    "\n",
    "# Create individual EUNIS level columns in fact_tbl using a lambda function and apply() method on 'EUNIS_Code' column on DF.\n",
    "fact_tbl[['Level_1', 'Level_2', 'Level_3',\n",
    "          'Level_4', 'Level_5', 'Level_6']] = fact_tbl.apply(lambda row: pd.Series(eunis_col(row)), axis=1)\n",
    "\n",
    "# Create new 'EUNIS_Level' column which indicates the numerical value of the EUNIS level by passing the fact_tbl to the \n",
    "# eunis_lvl() function. \n",
    "fact_tbl['EUNIS_Level'] = fact_tbl.apply(lambda row: eunis_lvl(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Level 6 to 5 subsetting data\n",
    "\n",
    "The following section of code performs the aggregation of data from EUNIS Level 6 to EUNIS Level 5.\n",
    "All data which have undergone an aggregation comprise biotopes which have assessments at Level 6, but not at Level 5. Therefore this will allow the end user to identify which resistance scores are relevant to lower EUNIS levels when an assessment has not been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all level 6 data only and assign to object oriented variable to be aggregated to level 5\n",
    "L6 = pd.DataFrame(fact_tbl.loc[fact_tbl['EUNIS_Level'].isin(['6'])])\n",
    "\n",
    "# Extract all original level 5 data and assign to object oriented variable\n",
    "L5_orig = pd.DataFrame(fact_tbl.loc[fact_tbl['EUNIS_Level'].isin(['5'])])\n",
    "\n",
    "# Assign data differences to new object oriented variable using outer merge between data frames\n",
    "L56_merge = pd.merge(L6, L5_orig, how='outer', on=['Level_5', 'Pressure'], indicator=True)\n",
    "\n",
    "# Filter merged data by left only data to subset EUNIS level 6 data which does not have a level 5 assessment\n",
    "L56_diff = pd.DataFrame(L56_merge[L56_merge['_merge'] == 'left_only'])\n",
    "\n",
    "#       Clean outer merge of excess data from right only channel\n",
    "L56_diff = L56_diff.drop([\n",
    "    'ID_x_y', 'SubregionName_y', 'RegionName_y', 'BiotopePresence_y', 'EUNIS_Code_y',\n",
    "    'HabitatName_y', 'Gaps_y', 'ID_y_y', 'Name_y', 'NE_Code_y', 'Resistance_y', 'ResistanceQoE_y',\n",
    "    'ResistanceAoE_y', 'ResistanceDoE_y', 'Resilience_y', 'ResilienceQoE_y', 'ResilienceAoE_y',\n",
    "    'resilienceDoE_y', 'Sensitivity_y', 'SensitivityQoE_y', 'SensitivityAoE_y', 'SensitivityDoE_y',\n",
    "    'url_y', 'Level_1_y', 'Level_2_y', 'Level_3_y', 'Level_4_y', 'Level_6_y', 'EUNIS_Level_y',\n",
    "    '_merge'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4a: Level 6 to 5 aggregation (formatting)\n",
    "\n",
    "The following body of code begins the initial steps of the aggregation process from level 6 to level 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by Level_5, Pressure, SubregionName and apply resistance values to list using lambdas function and .apply() method\n",
    "L5_agg = L56_diff.groupby(['Level_5', 'Pressure', 'SubregionName_x']\n",
    "                          )['Resistance_x'].apply(lambda x: ', '.join(x))\n",
    "\n",
    "# Convert the Pandas Series Object into a DataFrame to be manipulated later in the script\n",
    "L5_agg = pd.DataFrame(L5_agg)\n",
    "\n",
    "# Reset index of newly created DataFrame to pull out data into 4 individual columns\n",
    "L5_agg = L5_agg.reset_index(inplace=False)\n",
    "\n",
    "# Reset columns within L5_agg DF \n",
    "L5_agg.columns = ['Level_5', 'Pressure', 'SubregionName', 'Resistance']\n",
    "\n",
    "# Apply the counter() function to the DF to count the occurrence of all assessment values\n",
    "L5_agg[['High', 'Medium', 'Low', 'Not sensitive', 'Not relevant', 'No evidence', 'Not assessed',\n",
    "        'Unknown']] = L5_agg.apply(lambda df: pd.Series(counter(df['Resistance'])), axis=1)\n",
    "\n",
    "# Duplicate all count values and assign to new columns to be replaced by string values later in code \n",
    "L5_agg['Count_High'] = L5_agg['High']\n",
    "L5_agg['Count_Medium'] = L5_agg['Medium']\n",
    "L5_agg['Count_Low'] = L5_agg['Low']\n",
    "L5_agg['Count_NotSensitive'] = L5_agg['Not sensitive']\n",
    "L5_agg['Count_NotRel'] = L5_agg['Not relevant']\n",
    "L5_agg['Count_NoEvidence'] = L5_agg['No evidence']\n",
    "L5_agg['Count_NotAssessed'] = L5_agg['Not assessed']\n",
    "L5_agg['Count_Unknown'] = L5_agg['Unknown']\n",
    "\n",
    "# Reassign L5_agg DataFrame to L5_sens for sensitivity aggregation\n",
    "L5_res = L5_agg\n",
    "\n",
    "# Create colNames list for use with replacer() function\n",
    "colNames = ['High', 'Medium', 'Low', 'Not sensitive', 'Not relevant', 'No evidence', 'Not assessed', 'Unknown']\n",
    "\n",
    "# Run replacer() function on one set of newly duplicated columns to convert integers to string values of the assessment score\n",
    "for eachCol in colNames:\n",
    "    L5_res[eachCol] = L5_res[eachCol].apply(lambda x: replacer(x, eachCol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4b: Level 6 to 5 aggregation (aggregation)\n",
    "\n",
    "Aggregate data using the functions defined in section 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use lambda function to apply create_Resistance() function to each row within the DataFrame\n",
    "L5_res['L5_Resistance'] = L5_res.apply(lambda df: create_Resistance(df), axis=1)\n",
    "\n",
    "# Use lambda function to apply final_Resistance() function to each row within the DataFrame\n",
    "L5_res['L5_FinalResistance'] = L5_res.apply(lambda df: final_Resistance(df), axis=1)\n",
    "\n",
    "# Use lambda function to apply combine_assessedcounts() function to each row within the DataFrame\n",
    "L5_res['L5_AssessedCount'] = L5_res.apply(lambda df: combine_assessedcounts(df), axis=1)\n",
    "\n",
    "# Use lambda function to apply combine_unassessedcounts() function to each row within the DataFrame\n",
    "L5_res['L5_UnassessedCount'] = L5_res.apply(lambda df: combine_unassessedcounts(df), axis=1)\n",
    "\n",
    "# Apply column4() function to L5_res DataFrame to create new Level_4 column \n",
    "L5_res['Level_4'] = L5_res.apply(lambda df: pd.Series(column4(df)), axis=1)\n",
    "\n",
    "# Use lambda function to apply create_confidence() function to the DataFrame\n",
    "L5_res['L5_AssessmentConfidence'] = L5_res.apply(lambda df: create_confidence(df), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4c: Level 6 to 5 aggregation (combine aggregated and existing assessments)\n",
    "\n",
    "Format and combine existing non-aggregated values with newly computed outputs from aggregation process (Section 4b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object oriented variables to be used within section 4c (original L5 data and L5 aggregated data)\n",
    "L5_agg_res = L5_res\n",
    "L5_orig_sub = L5_orig\n",
    "\n",
    "#       Drop unwanted columns to allow for both DataFrames to have matching indices\n",
    "L5_agg_res = L5_agg_res.drop(['Resistance', 'Unknown', 'High', 'Medium', 'Low', 'Not sensitive', 'Not relevant',\n",
    "                                'No evidence', 'Not assessed', 'Count_High', 'Count_Medium', 'Count_Low',\n",
    "                                'Count_NotSensitive', 'Count_NotRel', 'Count_NoEvidence',\n",
    "                                'Count_NotAssessed', 'Count_Unknown'], axis=1, inplace=False)\n",
    "\n",
    "# Rename newly created L5_sensitivity column 'Sensitivity' to match both DataFrames \n",
    "L5_agg_res.rename(columns={'L5_Resistance': 'Resistance'}, inplace=True)\n",
    "\n",
    "# Drop unwanted columns from 'L5_orig_sub' DataFrame\n",
    "L5_orig_sub = L5_orig_sub.drop(['ID_x', 'RegionName', 'BiotopePresence', 'EUNIS_Code', 'HabitatName', 'Gaps', 'ID_y',\n",
    "                                'Name',\n",
    "                                'NE_Code', 'Resilience', 'ResistanceQoE', 'ResistanceAoE', 'ResistanceDoE',\n",
    "                                'Sensitivity', 'ResilienceQoE', 'ResilienceAoE', 'resilienceDoE', 'SensitivityQoE',\n",
    "                                'SensitivityAoE', 'SensitivityDoE', 'url', 'Level_1', 'Level_2', 'Level_3', 'Level_4',\n",
    "                                'Level_6', 'EUNIS_Level', 'JNCC_Code'], axis=1, inplace=False)\n",
    "\n",
    "\n",
    "# Apply the counter() function to the L5_orig_sub DataFrame to count the occurrence of all assessment values\n",
    "L5_orig_sub[['High', 'Medium', 'Low', 'Not sensitive', 'Not relevant', 'No evidence', 'Not assessed',\n",
    "             'Unknown']] = L5_orig_sub.apply(lambda df: pd.Series(counter(df['Resistance'])), axis=1)\n",
    "\n",
    "# Duplicate all L5_orig_sub count values and assign to new columns to be replaced by string values later in code \n",
    "L5_orig_sub['Count_High'] = L5_orig_sub['High']\n",
    "L5_orig_sub['Count_Medium'] = L5_orig_sub['Medium']\n",
    "L5_orig_sub['Count_Low'] = L5_orig_sub['Low']\n",
    "L5_orig_sub['Count_NotSensitive'] = L5_orig_sub['Not sensitive']\n",
    "L5_orig_sub['Count_NotRel'] = L5_orig_sub['Not relevant']\n",
    "L5_orig_sub['Count_NoEvidence'] = L5_orig_sub['No evidence']\n",
    "L5_orig_sub['Count_NotAssessed'] = L5_orig_sub['Not assessed']\n",
    "L5_orig_sub['Count_Unknown'] = L5_orig_sub['Unknown']\n",
    "\n",
    "# Run replacer() function on one set of newly duplicated columns to convert integers to string values of the assessment score\n",
    "for eachCol in colNames:\n",
    "    L5_orig_sub[eachCol] = L5_orig_sub[eachCol].apply(lambda x: replacer(x, eachCol))\n",
    "\n",
    "# Use lambda function to apply final_Resistance() function to each row within the DataFrame\n",
    "L5_orig_sub['L5_FinalResistance'] = L5_orig_sub.apply(lambda df: final_Resistance(df), axis=1)\n",
    "\n",
    "# Use lambda function to apply combine_assessedcounts() function to each row within the DataFrame\n",
    "L5_orig_sub['L5_AssessedCount'] = L5_orig_sub.apply(lambda df: combine_assessedcounts(df), axis=1)\n",
    "\n",
    "# Use lambda function to apply combine_unassessedcounts() function to each row within the DataFrame\n",
    "L5_orig_sub['L5_UnassessedCount'] = L5_orig_sub.apply(lambda df: combine_unassessedcounts(df), axis=1)\n",
    "\n",
    "# Apply column4() function to L5_orig_sub DataFrame to create new Level_4 column\n",
    "L5_orig_sub['Level_4'] = L5_orig_sub.apply(lambda df: pd.Series(column4(df)), axis=1)\n",
    "\n",
    "# Use lambda function to apply create_confidence() function to the DataFrame\n",
    "L5_orig_sub['L5_AssessmentConfidence'] = L5_orig_sub.apply(lambda df: create_confidence(df), axis=1)\n",
    "\n",
    "# Drop all unwanted columns from L5_orig_sub DataFrame\n",
    "L5_orig_sub.drop(['High', 'Medium', 'Low', 'Not sensitive', 'Not relevant', 'No evidence',\n",
    "                  'Not assessed', 'Unknown', 'Count_High', 'Count_Medium', 'Count_Low', 'Count_NotSensitive',\n",
    "                  'Count_NotRel', 'Count_NoEvidence', 'Count_NotAssessed', 'Count_Unknown',\n",
    "                  'JNCC_Name'], axis=1, inplace=True)\n",
    "\n",
    "# Append L5_orig_sub DataFrame with newly developed L5_agg_sens DataFrame\n",
    "L5_all = L5_orig_sub.append(L5_agg_res, sort=True)\n",
    "\n",
    "# Drop 'Classification level' column as this is not currently used in the aggregation process.\n",
    "# L5_all.drop(['Classification_Level'], axis=1, inplace=True)\n",
    "\n",
    "# Format columns into correct order\n",
    "L5_all = L5_all[['Pressure', 'SubregionName', 'Level_4', 'Level_5', 'Resistance', 'L5_FinalResistance',\n",
    "                 'L5_AssessedCount', 'L5_UnassessedCount', 'L5_AssessmentConfidence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4d: Level 6 to 5 aggregation (creating an aggregated export)\n",
    "\n",
    "Export aggregated data to be combined into the MasterFrame at the end of the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for master DataFrame at end of script \n",
    "L5_export = L5_all\n",
    "\n",
    "# Drop unwanted columns from L5_export DataFrame\n",
    "L5_export = L5_export.drop(['Resistance'], axis=1, inplace=False)\n",
    "\n",
    "# Rename and reorder columns within L5_export\n",
    "L5_export = L5_export[['Level_4', 'Pressure', 'SubregionName', 'Level_5', 'L5_FinalResistance',\n",
    "                         'L5_AssessedCount', 'L5_UnassessedCount', 'L5_AssessmentConfidence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5a: Level 5 to 4 aggregation (formatting)\n",
    "\n",
    "The following body of code begins the initial steps of the aggregation process from level 5 to level 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by Level_4, Pressure, SubregionName and apply resistance values to list using lambdas function and .apply() method\n",
    "L4_agg = L5_all.groupby(['Level_4', 'Pressure', 'SubregionName'\n",
    "                         ])['Resistance'].apply(lambda x: ', '.join(x))\n",
    "\n",
    "# Convert the Pandas Series Object into a DataFrame to be manipulated later in the script\n",
    "L4_agg = pd.DataFrame(L4_agg)\n",
    "\n",
    "# Reset index of newly created DataFrame to pull out data into 4 individual columns\n",
    "L4_agg = L4_agg.reset_index(inplace=False)\n",
    "\n",
    "# Reset columns within L4_agg DataFrame\n",
    "L4_agg.columns = ['Level_4', 'Pressure', 'SubregionName', 'Resistance']\n",
    "\n",
    "# Apply the counter() function to the DataFrame to count the occurrence of all assessment values\n",
    "L4_agg[['High', 'Medium', 'Low', 'Not sensitive', 'Not relevant', 'No evidence', 'Not assessed',\n",
    "        'Unknown']] = L4_agg.apply(lambda df: pd.Series(counter(df['Resistance'])), axis=1)\n",
    "\n",
    "# Duplicate all count values and assign to new columns to be replaced by string values later\n",
    "L4_agg['Count_High'] = L4_agg['High']\n",
    "L4_agg['Count_Medium'] = L4_agg['Medium']\n",
    "L4_agg['Count_Low'] = L4_agg['Low']\n",
    "L4_agg['Count_NotSensitive'] = L4_agg['Not sensitive']\n",
    "L4_agg['Count_NotRel'] = L4_agg['Not relevant']\n",
    "L4_agg['Count_NoEvidence'] = L4_agg['No evidence']\n",
    "L4_agg['Count_NotAssessed'] = L4_agg['Not assessed']\n",
    "L4_agg['Count_Unknown'] = L4_agg['Unknown']\n",
    "\n",
    "# Reassign L4_agg DataFrame to L4_sens for sensitivity aggregation\n",
    "L4_res = L4_agg\n",
    "\n",
    "# Run replacer() function on one set of newly duplicated columns to convert integers to string values of the assessment score\n",
    "for eachCol in colNames:\n",
    "    L4_res[eachCol] = L4_res[eachCol].apply(lambda x: replacer(x, eachCol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5b: Level 5 to 4 aggregation (aggregation)\n",
    "\n",
    "Aggregate data using the functions defined in section 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use lambda function to apply create_Resistance() function to each row within the DataFrame\n",
    "L4_res['L4_Resistance'] = L4_res.apply(lambda df: create_Resistance(df), axis=1)\n",
    "\n",
    "# Use lambda function to apply final_Resistance() function to each row within the DataFrame\n",
    "L4_res['L4_FinalResistance'] = L4_res.apply(lambda df: final_Resistance(df), axis=1)\n",
    "\n",
    "# Use lambda function to apply combine_assessedcounts() function to each row within the DataFrame\n",
    "L4_res['L4_AssessedCount'] = L4_res.apply(lambda df: combine_assessedcounts(df), axis=1)\n",
    "\n",
    "# Use lambda function to apply combine_unassessedcounts() function to each row within the DataFrame\n",
    "L4_res['L4_UnassessedCount'] = L4_res.apply(lambda df: combine_unassessedcounts(df), axis=1)\n",
    "\n",
    "# Apply column3() function to L4_sens DataFrame to create new Level_3 column \n",
    "L4_res['Level_3'] = L4_res.apply(lambda df: pd.Series(column3(df, 'Level_4')), axis=1)\n",
    "\n",
    "# Use lambda function to apply create_confidence() function to the DataFrame\n",
    "L4_res['L4_AssessmentConfidence'] = L4_res.apply(lambda df: create_confidence(df), axis=1)\n",
    "\n",
    "# Format columns into correct order\n",
    "L4_res = L4_res[['Level_3', 'Pressure', 'SubregionName', 'Level_4', 'Resistance', 'L4_Resistance',\n",
    "                   'L4_FinalResistance', 'L4_AssessedCount', 'L4_UnassessedCount', 'L4_AssessmentConfidence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5c: Level 5 to 4 aggregation (creating an aggregated export)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for master DataFrame at end of script \n",
    "L4_export = L4_res\n",
    "\n",
    "# Drop unwanted columns from L4_export DataFrame\n",
    "L4_export = L4_export.drop(['Resistance', 'L4_Resistance'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6a: Level 4 to 3 aggregation (formatting)\n",
    "\n",
    "The following body of code begins the initial steps of the aggregation process from level 4 to level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by Level_3, Pressure, SubregionName and apply resistance values to list using lambdas function and .apply() method\n",
    "L3_agg = L4_res.groupby(['Level_3', 'Pressure', 'SubregionName'\n",
    "                          ])['Resistance'].apply(lambda x: ', '.join(x))\n",
    "\n",
    "# Convert the Pandas Series Object into a DataFrame to be manipulated later in the script\n",
    "L3_agg = pd.DataFrame(L3_agg)\n",
    "\n",
    "# Reset index of newly created DataFrame to pull out data into 4 individual columns\n",
    "L3_agg = L3_agg.reset_index(inplace=False)\n",
    "\n",
    "# Reset columns within L3_agg DataFrame\n",
    "L3_agg.columns = ['Level_3', 'Pressure', 'SubregionName', 'Resistance']\n",
    "\n",
    "# Apply the counter() function to the DataFrame to count the occurrence of all assessment values\n",
    "L3_agg[['High', 'Medium', 'Low', 'Not sensitive', 'Not relevant', 'No evidence', 'Not assessed',\n",
    "        'Unknown']] = L3_agg.apply(lambda df: pd.Series(counter(df['Resistance'])), axis=1)\n",
    "\n",
    "# Duplicate all count values and assign to new columns to be replaced by string values later\n",
    "L3_agg['Count_High'] = L3_agg['High']\n",
    "L3_agg['Count_Medium'] = L3_agg['Medium']\n",
    "L3_agg['Count_Low'] = L3_agg['Low']\n",
    "L3_agg['Count_NotSensitive'] = L3_agg['Not sensitive']\n",
    "L3_agg['Count_NotRel'] = L3_agg['Not relevant']\n",
    "L3_agg['Count_NoEvidence'] = L3_agg['No evidence']\n",
    "L3_agg['Count_NotAssessed'] = L3_agg['Not assessed']\n",
    "L3_agg['Count_Unknown'] = L3_agg['Unknown']\n",
    "\n",
    "# Reassign L3_agg DataFrame to L3_sens for sensitivity aggregation\n",
    "L3_res = L3_agg\n",
    "\n",
    "# Run replacer() function on one set of newly duplicated columns to convert integers to string values of the assessment score\n",
    "for eachCol in colNames:\n",
    "    L3_res[eachCol] = L3_res[eachCol].apply(lambda x: replacer(x, eachCol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6b: Level 4 to 3 aggregation (aggregation)\n",
    "\n",
    "Aggregate data using the functions defined in section 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use lambda function to apply create_Resistance() function to each row within the DataFrame\n",
    "L3_res['L3_Resistance'] = L3_res.apply(lambda df: create_Resistance(df), axis=1)\n",
    "\n",
    "# Use lambda function to apply final_Resistance() function to each row within the DataFrame\n",
    "L3_res['L3_FinalResistance'] = L3_res.apply(lambda df: final_Resistance(df), axis=1)\n",
    "\n",
    "# Use lambda function to apply combine_assessedcounts() function to each row within the DataFrame\n",
    "L3_res['L3_AssessedCount'] = L3_res.apply(lambda df: combine_assessedcounts(df), axis=1)\n",
    "\n",
    "# Use lambda function to apply combine_unassessedcounts() function to each row within the DataFrame\n",
    "L3_res['L3_UnassessedCount'] = L3_res.apply(lambda df: combine_unassessedcounts(df), axis=1)\n",
    "\n",
    "# Apply column2() function to L3_res DataFrame to create new Level_2 column \n",
    "L3_res['Level_2'] = L3_res.apply(lambda df: pd.Series(column2(df, 'Level_3')), axis=1)\n",
    "\n",
    "# Use lambda function to apply create_confidence() function to the DataFrame\n",
    "L3_res['L3_AssessmentConfidence'] = L3_res.apply(lambda df: create_confidence(df), axis=1)\n",
    "\n",
    "# Drop unwanted data from L3_sens DataFrame\n",
    "L3_res = L3_res.drop(['Unknown', 'High', 'Medium', 'Low', 'Not sensitive', 'Not relevant',\n",
    "                        'No evidence', 'Not assessed', 'Count_High', 'Count_Medium', 'Count_Low',\n",
    "                        'Count_NotSensitive', 'Count_NotRel', 'Count_NoEvidence',\n",
    "                        'Count_NotAssessed', 'Count_Unknown'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6c: Level 4 to 3 aggregation (creating an aggregated export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for master DataFrame at end of script \n",
    "L3_export = L3_res\n",
    "\n",
    "# Drop unwanted columns from L3_export DataFrame\n",
    "L3_export = L3_export.drop(['L3_Resistance', 'Resistance'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7a: Level 3 to 2 aggregation (formatting)\n",
    "\n",
    "The following body of code begins the initial steps of the aggregation process from level 3 to level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by Level_2, Pressure, SubregionName and apply resistance values to list using lambdas function and .apply() method\n",
    "L2_agg = L3_res.groupby(['Level_2', 'Pressure', 'SubregionName'\n",
    "                          ])['Resistance'].apply(lambda x: ', '.join(x))\n",
    "\n",
    "# Convert the Pandas Series Object into a DataFrame to be manipulated later in the script\n",
    "L2_agg = pd.DataFrame(L2_agg)\n",
    "\n",
    "# Reset index of newly created DataFrame to pull out data into 4 individual columns\n",
    "L2_agg = L2_agg.reset_index(inplace=False)\n",
    "\n",
    "# Reset columns within L2_agg DataFrame\n",
    "L2_agg.columns = ['Level_2', 'Pressure', 'SubregionName', 'Resistance']\n",
    "\n",
    "# Apply the counter() function to the DataFrame to count the occurrence of all assessment values\n",
    "L2_agg[['High', 'Medium', 'Low', 'Not sensitive', 'Not relevant', 'No evidence', 'Not assessed',\n",
    "        'Unknown']] = L2_agg.apply(lambda df: pd.Series(counter(df['Resistance'])), axis=1)\n",
    "\n",
    "# Duplicate all count values and assign to new columns to be replaced by string values later\n",
    "L2_agg['Count_High'] = L2_agg['High']\n",
    "L2_agg['Count_Medium'] = L2_agg['Medium']\n",
    "L2_agg['Count_Low'] = L2_agg['Low']\n",
    "L2_agg['Count_NotSensitive'] = L2_agg['Not sensitive']\n",
    "L2_agg['Count_NotRel'] = L2_agg['Not relevant']\n",
    "L2_agg['Count_NoEvidence'] = L2_agg['No evidence']\n",
    "L2_agg['Count_NotAssessed'] = L2_agg['Not assessed']\n",
    "L2_agg['Count_Unknown'] = L2_agg['Unknown']\n",
    "\n",
    "# Reassign L2_agg DataFrame to L2_sens for sensitivity aggregation\n",
    "L2_res = L2_agg\n",
    "\n",
    "# Run replacer() function on one set of newly duplicated columns to convert integers to string values of the assessment score\n",
    "for eachCol in colNames:\n",
    "    L2_res[eachCol] = L2_res[eachCol].apply(lambda x: replacer(x, eachCol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7b: Level 3 to 2 aggregation (aggregation)\n",
    "\n",
    "Aggregate data using the functions defined in section 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use lambda function to apply create_Resistance() function to each row within the DataFrame\n",
    "L2_res['L2_Resistance'] = L2_res.apply(lambda df: create_Resistance(df), axis=1)\n",
    "\n",
    "# Use lambda function to apply final_Resistance() function to each row within the DataFrame\n",
    "L2_res['L2_FinalResistance'] = L2_res.apply(lambda df: final_Resistance(df), axis=1)\n",
    "\n",
    "# Use lambda function to apply combine_assessedcounts() function to each row within the DataFrame\n",
    "L2_res['L2_AssessedCount'] = L2_res.apply(lambda df: combine_assessedcounts(df), axis=1)\n",
    "\n",
    "# Use lambda function to apply combine_unassessedcounts() function to each row within the DataFrame\n",
    "L2_res['L2_UnassessedCount'] = L2_res.apply(lambda df: combine_unassessedcounts(df), axis=1)\n",
    "\n",
    "# Use lambda function to apply create_confidence() function to the DataFrame\n",
    "L2_res['L2_AssessmentConfidence'] = L2_res.apply(lambda df: create_confidence(df), axis=1)\n",
    "\n",
    "# Drop unwanted data from L2_sens DataFrame \n",
    "L2_res = L2_res.drop(['Resistance', 'Unknown', 'High', 'Medium', 'Low', 'Not sensitive', 'Not relevant',\n",
    "                        'No evidence', 'Not assessed', 'Count_High', 'Count_Medium', 'Count_Low',\n",
    "                        'Count_NotSensitive', 'Count_NotRel', 'Count_NoEvidence',\n",
    "                        'Count_NotAssessed', 'Count_Unknown'], axis=1, inplace=False)\n",
    "\n",
    "# Format columns into correct order\n",
    "L2_res = L2_res[['Level_2', 'Pressure', 'SubregionName', 'L2_FinalResistance', 'L2_Resistance',\n",
    "                   'L2_AssessedCount', 'L2_UnassessedCount', 'L2_AssessmentConfidence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7c: Level 3 to 2 aggregation (creating an aggregated export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for master DataFrame at end of script \n",
    "L2_export = L2_res\n",
    "\n",
    "# Drop unwanted columns from L2_export DataFrame\n",
    "L2_export = L2_export.drop(['L2_Resistance'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 8: Creating a MasterFrame\n",
    "\n",
    "Combine all exported DataFrames into one MasterFrame for export of aggregation work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge EUNIS Levels 2 and 3 \n",
    "L2L3 = pd.merge(L2_export, L3_export)\n",
    "\n",
    "# Merge EUNIS Levels 3 and 4\n",
    "L3L4 = pd.merge(L2L3, L4_export)\n",
    "\n",
    "# Merge EUNIS Levels 4 and 5\n",
    "MasterFrame = pd.merge(L3L4, L5_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 9: Categorising confidence values \n",
    "\n",
    "Assess all confidence scores using the categorise_confidence() function developed in Section 2b and store information in a correlating Confidence Category column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categories for confidence values: EUNIS Level 5\n",
    "MasterFrame['L5_ConfidenceCategory'] = MasterFrame.apply(\n",
    "    lambda df: categorise_confidence(df, 'L5_AssessmentConfidence'), axis=1)\n",
    "\n",
    "# Create categories for confidence values: EUNIS Level 4\n",
    "MasterFrame['L4_ConfidenceCategory'] = MasterFrame.apply(\n",
    "    lambda df: categorise_confidence(df, 'L4_AssessmentConfidence'), axis=1)\n",
    "\n",
    "# Create categories for confidence values: EUNIS Level 3\n",
    "MasterFrame['L3_ConfidenceCategory'] = MasterFrame.apply(\n",
    "    lambda df: categorise_confidence(df, 'L3_AssessmentConfidence'), axis=1)\n",
    "\n",
    "# Create categories for confidence values: EUNIS Level 2\n",
    "MasterFrame['L2_ConfidenceCategory'] = MasterFrame.apply(\n",
    "    lambda df: categorise_confidence(df, 'L2_AssessmentConfidence'), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 10: Exporting the MasterFrame \n",
    "\n",
    "Review the newly developed MasterFrame, and export to a .csv format file. \n",
    "To export the data, utilise the export code which is stored as a comment (#) - ensure that you select an appropriate file path when completing this stage.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MasterFrame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-868eac43831e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#       Export MasterFrame in CSV format  - Offshore Only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m MasterFrame.to_csv('INSERT FILEPATH HERE \\INSERT FILE NAME HERE.csv',\n\u001b[0m\u001b[0;32m      3\u001b[0m                    sep=',')\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#       Export MasterFrame in CSV format - All - Inshore & Offshore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MasterFrame' is not defined"
     ]
    }
   ],
   "source": [
    "# Create correct order for columns within MasterFrame\n",
    "MasterFrame = MasterFrame[['Pressure', 'SubregionName', 'Level_2', 'L2_FinalResistance', 'L2_AssessedCount',\n",
    "                           'L2_UnassessedCount', 'L2_AssessmentConfidence', 'L2_ConfidenceCategory', 'Level_3',\n",
    "                           'L3_FinalResistance', 'L3_AssessedCount', 'L3_UnassessedCount', 'L3_AssessmentConfidence',\n",
    "                           'L3_ConfidenceCategory', 'Level_4', 'L4_FinalResistance', 'L4_AssessedCount',\n",
    "                           'L4_UnassessedCount', 'L4_AssessmentConfidence', 'L4_ConfidenceCategory', 'Level_5',\n",
    "                           'L5_FinalResistance', 'L5_AssessedCount', 'L5_UnassessedCount', 'L5_AssessmentConfidence',\n",
    "                           'L5_ConfidenceCategory']]\n",
    "\n",
    "#       Export MasterFrame in CSV format  - Offshore Only\n",
    "MasterFrame.to_csv('INSERT FILEPATH HERE \\INSERT FILE NAME HERE.csv',\n",
    "                   sep=',')\n",
    "\n",
    "#       Export MasterFrame in CSV format - All - Inshore & Offshore\n",
    "MasterFrame.to_csv('INSERT FILEPATH HERE \\INSERT FILE NAME HERE.csv',\n",
    "                  sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
